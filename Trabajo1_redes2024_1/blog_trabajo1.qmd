---
title: "Optimización heurística"
author:
  - name: Ronald Gabriel Palencia
    email: ropalencia@unal.edu.co
  - name: Junior Antonio Muños Henao
    email: jmunozhe@unal.edu.co

format:
  html:
    code-fold: true
jupyter: python3
echo: false
theme:
          light: flatly
          dark: darkly
toc: true
appendix-style: default
---
# **Optimzación Heurìstica**

## Introdución


La optimización heurística engloba un conjunto de métodos destinados a encontrar soluciones aproximadas a problemas de optimización que son demasiado complejos para los enfoques analíticos tradicionales, como aquellos basados en el cálculo de gradientes. A diferencia de los métodos determinísticos, que procuran soluciones exactas bajo supuestos matemáticos estrictos sobre las funciones objetivo, los enfoques heurísticos exploran el espacio de soluciones de manera estratégica para encontrar soluciones satisfactorias sin garantizar necesariamente el óptimo global. Estos enfoques resultan particularmente valiosos en situaciones donde la función objetivo es discontinua, no diferenciable, altamente compleja, o cuando el espacio de búsqueda es tan amplio que hace inviable una exploración exhaustiva.

Dentro de los métodos heurísticos más destacados y empleados se hallan los algoritmos evolutivos y las colonias de hormigas, ambos inspirados en fenómenos biológicos.

**Algoritmos Evolutivos**

Los algoritmos evolutivos son inspirados por el proceso de evolución biológica y selección natural, operando mediante un ciclo de selección, cruzamiento, y mutación en una población de soluciones candidatas (Mitchell, 1998). Cada solución es evaluada a través de una función de aptitud, determinando su probabilidad de ser seleccionada para la generación de nuevas soluciones. Este ciclo permite que, con el tiempo, las soluciones más aptas predominen, dirigiendo al algoritmo hacia una solución óptima o satisfactoria.

**Colonias de Hormigas**

Por su parte, las colonias de hormigas simulan el comportamiento colectivo de las hormigas en su búsqueda de alimentos, donde cada hormiga explora el espacio de soluciones siguiendo caminos marcados por feromonas, sustancias químicas que permiten la comunicación entre ellas (Dorigo, Maniezzo, & Colorni, 1996). Las rutas con concentraciones más altas de feromonas atraen a más hormigas, favoreciendo la exploración de soluciones prometedoras a través de un proceso de realimentación positiva.

**Aplicación de Métodos Heurísticos**

La aplicación de métodos heurísticos para la optimización de funciones de prueba implica varios pasos clave, comenzando por la definición precisa de la función de prueba que representa el problema de optimización. La elección del método heurístico adecuado depende de las características del problema, incluyendo la naturaleza de la función objetivo y las peculiaridades del espacio de búsqueda. La configuración de los parámetros del algoritmo, como el tamaño de la población en los algoritmos evolutivos o el número de hormigas en las colonias de hormigas, es fundamental para la efectividad del método. Posteriormente, se ejecuta el algoritmo y se monitorea su progreso, realizando ajustes en los parámetros según sea necesario para optimizar los resultados.

El empleo de métodos heurísticos en la optimización presenta una alternativa robusta y adaptable para abordar problemas complejos, ofreciendo soluciones aproximadas cuando los enfoques convencionales resultan inadecuados o ineficaces. La inspiración en procesos naturales no solo proporciona una perspectiva innovadora para la resolución de problemas de optimización sino que también abre nuevas posibilidades de investigación y aplicación en áreas como ingeniería, economía, y ciencia de datos.





## **Parte 1: optimización numérica**


Considere las siguientes funciones de prueba:

**Función de Rosenbrock**

La **Función de Rosenbrock** es conocida por su valle en forma de banana que contiene el mínimo global. Se define como:

$$
f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2
$$

con $x_i \in [-2.048, 2.048]$, $i = 1, 2$. Alcanza su valor mínimo en $x_1 = 1$ y $x_2 = 1$.

**Función de Rastrigin**

La **Función de Rastrigin** es una función no convexa utilizada como problema de prueba de rendimiento para algoritmos de optimización debido a su gran cantidad de mínimos locales. Se define como:

$$
f(x_1, x_2) = 20 + \sum_{i=1}^{2}(x_i^2 - 10 \cos(2\pi x_i))
$$

con $x_i \in [-5.12, 5.12]$, $i = 1, 2$. Alcanza su valor mínimo en $x_1 = 0$ y $x_2 = 0$.

**Función de Schwefel**

La **Función de Schwefel** es conocida por su complejidad y su paisaje de búsqueda desafiante. Se define como:

$$
f(x_1, x_2) = 418.9829 \times 2 - \sum_{i=1}^{2}x_i \sin(\sqrt{|x_i|})
$$

con $x_i \in [-500, 500]$, $i = 1, 2$. Alcanza su valor mínimo en $x_1 = 420.9687$ y $x_2 = 420.9687$.

**Función de Griewank**

La **Función de Griewank** a menudo se utiliza para probar la eficacia de algoritmos de optimización en tratar con oscilaciones grandes y numerosos óptimos locales. Se define como:

$$
f(x_1, x_2) = \frac{1}{4000}\sum_{i=1}^{2}x_i^2 - \prod_{i=1}^{2}\cos(\frac{x_i}{\sqrt{i}}) + 1
$$

con $x_i \in [-600, 600]$, $i = 1, 2$. Alcanza su valor mínimo en $x_1 = 0$ y $x_2 = 0$.

**Función Goldstein-Price**

La **Función Goldstein-Price** es conocida por tener un valle estrecho que conduce al mínimo global. Se define como:

$$
\begin{align*}
f(x_1, x_2) = & [1 + (x_1 + x_2 + 1)^2(19 - 14x_1 + 3x_1^2 - 14x_2 + 6x_1x_2 + 3x_2^2)]\\
& \times [30 + (2x_1 - 3x_2)^2(18 - 32x_1 + 12x_1^2 + 48x_2 - 36x_1x_2 + 27x_2^2)]
\end{align*}
$$

con $x_i \in [-2, 2]$, $i = 1, 2$. Alcanza su valor mínimo en $x_1 = 0$ y $x_2 = -1$.

**Función de las seis jorobas de camello (Six-Hump Camel Back)**

La **Función de las seis jorobas de camello** es una función de prueba que presenta dos mínimos globales. Se define como:

$$
f(x_1, x_2) = (4 - 2.1x_1^2 + \frac{x_1^4}{3})x_1^2 + x_1x_2 + (-4 + 4x_2^2)x_2^2
$$

con \(x_1 \in [-3, 3]\) y \(x_2 \in [-2, 2]\). Alcanza su valor mínimo en \(x_1 = -0.0898\) y \(x_2 = 0.7126\), y también en \(x_1 = 0.0898\) y \(x_2 = 0.7126\).


**Teniendo en cuanta las anteriores funciones realizar los siguientes items:**


1. Escoja dos funciones de prueba

2. Optimice las funciones en dos y tres dimensiones usando un método de descenso por gradiente con condición inicial aleatoria

3. Optimice las funciones en dos y tres dimensiones usando: algoritmos evolutivos, optimización de partículas y evolución diferencial

4. Represente con un gif animado o un video el proceso de optimización de descenso por gradiente y el proceso usando el método heurístico.


```{python}

#!pip install pygad
#!pip install numpy

import numpy as np

import matplotlib.pyplot as plt
from matplotlib import animation, rc
from IPython.display import HTML;
import pygad
rc('animation', html='html5');
```
## 1. Funciones de Prueba

**La funciones de prueba escogidas para probar los métodos de optimización serán las siguientes:**


* Función de Rastrigin

* Función de Schwefel


```{python}
# Se implementa la función de Rastrigin vectorizada para arreglos numpy
def Rastrigin(X):
  Y = 10 * len(X) + np.sum(X ** 2 - 10 * np.cos(2 * np.pi * X))
  return Y

# Se implementa la función de Schwefel vectorizada para arreglos numpy
def Schwefel(X):
    Y = np.sum(X * np.sin(np.sqrt(np.abs(X))))
    return(Y)

# Visualización de la función de Rastrigin en 2D
ncols = 100
nrows = 100
X1 = np.linspace(-5.12, 5.12, ncols)
Y1 = np.linspace(-5.12, 5.12, nrows)
X1, Y1 = np.meshgrid(X1, Y1)

Z1 = [Rastrigin(np.array([X1[i,j], Y1[i,j]])) for i in range(nrows) for j in range(ncols)]
Z1 = np.array(Z1).reshape([nrows,ncols])

fig, (ax1,ax2) = plt.subplots(1,2)
fig.set_size_inches(13,5)
contorno = ax1.contourf(X1,Y1,Z1)
ax1.set_title("Función de Rastrigin")
fig.colorbar(contorno, ax=ax1)
fig.subplots_adjust(wspace=1.5)

# Visualización de la función de Schwefel en 2D
X2 = np.linspace(-512, 512, ncols*10)
Y2 = np.linspace(-512, 512, nrows*10)
X2, Y2 = np.meshgrid(X2, Y2)
Z2 = [Schwefel(np.array([X2[i,j], Y2[i,j]])) for i in range(nrows*10) for j in range(ncols*10)]
Z2 = np.array(Z2).reshape([nrows*10,ncols*10])
contorno2 = ax2.contourf(X2,Y2,Z2)
fig.colorbar(contorno2, ax=ax2)
ax2.set_title("Función de Schewfel")


plt.tight_layout()
plt.show()
```


**Función de Rastrigin (izquierda):**

La gráfica muestra una superficie repleta de picos y valles regulares y simétricos, característicos de la Función de Rastrigin. Esta función es conocida por su gran número de mínimos locales que dificultan encontrar el mínimo global. Los colores más oscuros representan valores más bajos de la función, y los más claros, valores más altos. El patrón periódico de los mínimos locales es claramente visible y sugiere la complejidad de utilizar métodos de optimización que no se basan en gradientes para encontrar el óptimo global, que se sitúa en el centro de la gráfica donde se observa un mínimo profundo y distinto.

**Función de Schwefel (derecha):**

La gráfica para la Función de Schwefel muestra una estructura más compleja con una serie de anillos concéntricos que representan los niveles de la función. La Función de Schwefel es conocida por tener un mínimo global pronunciado, pero rodeado de zonas que llevan a mínimos locales subóptimos, lo cual puede engañar a los algoritmos de búsqueda. Los valores más bajos en esta función se indican con un color oscuro, ubicados cerca del centro de la gráfica, que es donde se encuentra el mínimo global. La amplia gama de colores refleja la amplia variedad de valores que la función puede tomar, lo que indica un paisaje de optimización desafiante con múltiples mínimos locales.


**Visualización en 3D**


```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Definición de las funciones
def Rastrigin(X):
    return 10 * len(X) + np.sum(X ** 2 - 10 * np.cos(2 * np.pi * X))

def Schwefel(X):
    return 418.9829 * len(X) - np.sum(X * np.sin(np.sqrt(np.abs(X))))

# Creación de la malla para graficar
ncols, nrows = 100, 100
X1 = np.linspace(-5.12, 5.12, ncols)
Y1 = np.linspace(-5.12, 5.12, nrows)
X1, Y1 = np.meshgrid(X1, Y1)
Z1 = np.array([Rastrigin(np.array([x, y])) for x, y in zip(np.ravel(X1), np.ravel(Y1))])
Z1 = Z1.reshape(X1.shape)

# Graficar Rastrigin en 3D
fig = plt.figure(figsize=(14, 7))

# Rastrigin
ax1 = fig.add_subplot(121, projection='3d')
ax1.plot_surface(X1, Y1, Z1, cmap='viridis')
ax1.set_title("Función de Rastrigin")
ax1.set_xlabel('X1')
ax1.set_ylabel('Y1')
ax1.set_zlabel('Z1')

# Ajuste para la función de Schwefel
ncols, nrows = 1000, 1000
X2 = np.linspace(-500, 500, ncols)
Y2 = np.linspace(-500, 500, nrows)
X2, Y2 = np.meshgrid(X2, Y2)
Z2 = np.array([Schwefel(np.array([x, y])) for x, y in zip(np.ravel(X2), np.ravel(Y2))])
Z2 = Z2.reshape(X2.shape)

# Schwefel
ax2 = fig.add_subplot(122, projection='3d')
ax2.plot_surface(X2, Y2, Z2, cmap='viridis')
ax2.set_title("Función de Schwefel")
ax2.set_xlabel('X2')
ax2.set_ylabel('Y2')
ax2.set_zlabel('Z2')

plt.tight_layout()
plt.show()

```

Las gráficas 3D muestran claramente la topología de las funciones de Rastrigin y Schwefel. Rastrigin tiene un paisaje ondulado con numerosos mínimos locales, mientras que Schwefel presenta un paisaje más suave con un mínimo global prominente. Estas visualizaciones resaltan la complejidad inherente a cada función


## 2. Optimización mediante descenso por gradiente



A continuación, presentamos las implementaciones de las funciones necesarias para calcular el gradiente. Además, introducimos el método de descenso por gradiente, una técnica de optimización que utilizaremos para hallar los mínimos de las funciones objetivo. Este enfoque iterativo nos permitirá afinar progresivamente nuestras estimaciones hacia el punto de óptimo global.




```{python}
# La siguiente función implementa el cálculo del gradiente usando 'list comprehension'
def num_grad(x, fun, h=0.01):
    return np.array([(fun(x+e*h) - fun(x-e*h)) / (2*h) for e in np.eye(len(x))])

# La siguiente función implmenta la optimización mediante descenso por gradiente
def optimizador_mult_numdev(f, x0, eta=0.01, tol=1e-6, max_iter=1e3):
    x = np.array(x0)
    iter = 0
    while np.linalg.norm(num_grad(x, f, h=0.01)) > tol and iter < max_iter:
        x = x - eta*num_grad(x, f, h=0.01)
        iter += 1
    return x
```

### 2.1 Función de Rastrigin

#### Generación del gráfico

Se procede a definir las características del gráfico que representará todo.


 ```{python}

 x0 = np.array([4*np.random.random()-1,4*np.random.random()-1])
 xs = [x0]

 for i in range(25):
    x_new = optimizador_mult_numdev(Rastrigin, x0)
    xs.append(x_new)
    x0 = x_new

 ```




```{python}
# Particionamiento del rango de cada variable
ncols = 100
nrows = 100
X = np.linspace(-3.12, 3.12, ncols)  #Posibles valores de X
Y = np.linspace(-3.12, 3.12, nrows)  #Posibles valores de Y
X, Y = np.meshgrid(X, Y)  #Definición del plano con posibles valores

# Evaluación de la función de Rastrigin
Z = [Rastrigin(np.array([X[i,j], Y[i,j]])) for i in range(nrows) for j in range(ncols)]
Z = np.array(Z).reshape([nrows,ncols])

fig, ax = plt.subplots()
fig.set_size_inches(10,7)
contorno = ax.contour(X,Y,Z)  #Curvas de nivel de la función de Rastrigin
contornof = ax.contourf(X,Y,Z) #Rellenando las curvas de nivel
line, = ax.plot(xs[-1][0], xs[-1][1], 'xr--',mec='b', markersize=10) #Creando la línea que contendrá los puntos
              # de la animación de Optimización
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Función de Rastrigin')
plt.colorbar(contornof) # Se muestra la barra lateral con la escala de valores
          # ¡¡Éste método depende de la creación de un objeto "contourf" !!
plt.show()
```



En el gráfico anterior, se destaca el punto que representa el mínimo encontrado a través del método de descenso por gradiente. A continuación, presentaremos un GIF animado que muestra el conjunto de soluciones halladas en cada iteración del proceso hasta alcanzar la solución óptima. Esta visualización animada brinda una perspectiva clara y dinámica de cómo el algoritmo de descenso por gradiente avanza iterativamente a través del espacio de soluciones, acercándose paso a paso al punto de mínimo global de la función de Rastrigin. Este enfoque no solo facilita la comprensión del proceso de optimización, sino que también ilustra de manera efectiva la eficacia del algoritmo en la búsqueda de soluciones óptimas.


```{python}



# Definición de la función graficadora
def graficar(frame):
    x, y = zip(*xs[:frame+1])
    line.set_data(x, y)
    return line,


# Animación final
ani = animation.FuncAnimation(fig, graficar, frames=len(xs), interval=300)
ani
```

### Optimización en 3D



La siguiente gráfica ilustra el punto mínimo de la función de Rastrigin en tres dimensiones. Es notable que este mínimo está destacado en color rojo.





```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def Rastrigin(x):
    A = 10
    return A * len(x) + sum([(xi ** 2 - A * np.cos(2 * np.pi * xi)) for xi in x])

def num_grad(x, fun, h=0.001):
    return np.array([(fun(x + e * h) - fun(x - e * h)) / (2 * h) for e in np.eye(len(x))])

def optimizador_mult_numdev(f, x0, eta=0.0001, tol=1e-6, max_iter=1e4):
    x = np.array(x0)
    iter = 0
    while np.linalg.norm(num_grad(x, f)) > tol and iter < max_iter:
        x = x - eta * num_grad(x, f)
        iter += 1
    return x

ncols, nrows = 100, 100
X1 = np.linspace(-5.12, 5.12, ncols)
Y1 = np.linspace(-5.12, 5.12, nrows)
X1, Y1 = np.meshgrid(X1, Y1)
Z1 = np.array([Rastrigin(np.array([x, y])) for x, y in zip(np.ravel(X1), np.ravel(Y1))])
Z1 = Z1.reshape(X1.shape)

# Ajustar el punto inicial si es necesario
xmin = optimizador_mult_numdev(Rastrigin, [-3.0, 3.0], eta=0.01, max_iter=10000)  
zmin = Rastrigin(xmin)

# Graficar
fig = plt.figure(figsize=(14, 7))
ax = fig.add_subplot(111, projection='3d')
surf = ax.plot_surface(X1, Y1, Z1, cmap='viridis', edgecolor='none', alpha=0.7)
ax.scatter(xmin[0], xmin[1], zmin, color='r', s=50, label=f'Punto mínimo encontrado en {xmin} con valor {zmin}')
ax.set_title("Función de Rastrigin con punto mínimo")
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.legend()

plt.show()

```




```{python, warning = F}
"""
def optimizador_mult_numdev(f, x0, eta=0.01, tol=1e-6, max_iter=1e3):
    x = np.array(x0)
    iter = 0
    history = [x.copy()]  # Almacenar historial de puntos
    while np.linalg.norm(num_grad(x, f)) > tol and iter < max_iter:
        x = x - eta * num_grad(x, f)
        history.append(x.copy())  # Guardar copia del punto actual
        iter += 1
    return x, history
xmin, history = optimizador_mult_numdev(Rastrigin, [0.0, 0.0])  # Obtener punto mínimo y historial

from matplotlib.animation import FuncAnimation

# Preparar la figura y los ejes para la animación
fig = plt.figure(figsize=(14, 7))
ax = fig.add_subplot(111, projection='3d')
ax.set_xlim([-5.12, 5.12])
ax.set_ylim([-5.12, 5.12])
ax.set_zlim([0, 80])

# Superficie de la función Rastrigin
ax.plot_surface(X1, Y1, Z1, cmap='viridis', edgecolor='none', alpha=0.7)

# Inicializar el punto que vamos a mover en la animación
point, = ax.plot([], [], [], 'ro', markersize=8)

# Función de inicialización para la animación (necesaria para FuncAnimation)
def init():
    point.set_data([], [])
    point.set_3d_properties([])
    return (point,)

# Función de animación que se llama en cada cuadro
def animate(i):
    # Actualizar la posición del punto con el historial de optimización
    x, y = history[i][0], history[i][1]
    z = Rastrigin(history[i])
    point.set_data(x, y)
    point.set_3d_properties(z, 'z')
    return (point,)

# Crear animación
ani = FuncAnimation(fig, animate, init_func=init, frames=len(history), interval=100, blit=False)
ani
"""
```



El comportamiento del optimizador de descenso por gradiente, partiendo de un punto inicial $(-3, 3)$ y moviéndose hacia aproximadamente $(-2.96, 2.96)$ con un valor de función de 41, evidencia las limitaciones al enfrentar la compleja función de Rastrigin. Esta mejora, sin alcanzar el mínimo global, subraya las dificultades debido a la naturaleza multimodal de Rastrigin, caracterizada por abundantes mínimos locales. El resultado demuestra la capacidad del optimizador para progresar en un espacio desafiante, utilizando información local del gradiente en su búsqueda de optimización.


### 2.2 Función de Schwefel

[texto del vínculo](https://)Con las funciones de derivadas parciales y 

de optimización ya implementadas, sólo resta recalcular los datos para la función de Schwefel y generar el gráfico.


```{python}
def optimizador_mult_numdev(f, x0, eta=0.0001, tol=1e-6, max_iter=1e4):
    x = np.array(x0)
    iter = 0
    while np.linalg.norm(num_grad(x, f)) > tol and iter < max_iter:
        x = x - eta * num_grad(x, f, h = 1e-4)
        iter += 1
    return x
np.random.seed(42)
# Implementación del método de descenso por gradiente
x0 = np.array([300*(2*np.random.random()-1),300*(2*np.random.random()-1)]) # generación de un punto aleatorio
#print(x0)
#xs = [x0]
for i in range(25):
    x_new = optimizador_mult_numdev(Schwefel, x0)
    xs.append(x_new)
    x0 = x_new

print(xs)
```

```{python}
# Particionamiento del rango de cada variable
ncols = 1000
nrows = 1000
X = np.linspace(-512, 512, ncols)  #Posibles valores de X
Y = np.linspace(-512, 512, nrows)  #Posibles valores de Y
X, Y = np.meshgrid(X, Y)  #Definición del plano con posibles valores

# Evaluación de la función de Rastrigin
Z = [Schwefel(np.array([X[i,j], Y[i,j]])) for i in range(nrows) for j in range(ncols)]
Z = np.array(Z).reshape([nrows,ncols])

fig, ax = plt.subplots()
fig.set_size_inches(10,7)
contorno = ax.contour(X,Y,Z)  #Curvas de nivel de la función de Rastrigin
contornof = ax.contourf(X,Y,Z) #Rellenando las curvas de nivel
line, = ax.plot(xs[-1][0], xs[1][1], 'xr--',mec='b') #Creando la línea que contendrá los puntos
              # de la animación de Optimización
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Función de Schwefel')
plt.colorbar(contornof) # Se muestra la barra lateral con la escala de valores
          # ¡¡Éste método depende de la creación de un objeto "contourf" !!
plt.show()
```

Durante la ejecución del optimizador de descenso por gradiente aplicado a la función de Schwefel, el punto de inicio aleatorio seleccionado fue $(-75.28, 270.43)$. El optimizador convergió hacia el punto $(-65.56, 302.46)$, que está significativamente alejado del mínimo global conocido de la función. Esta dificultad para alcanzar el mínimo global puede atribuirse a la presencia de numerosos mínimos locales en la función de Schwefel, un desafío común para los métodos de optimización basados en el gradiente. Además, se realizó una exploración variando el valor de $\eta$, observándose que la reducción de este parámetro resultaba en una ejecución más lenta del algoritmo. En una sección posterior, se ilustrará este proceso de optimización mediante un GIF, que ofrecerá una visualización de la trayectoria seguida por el optimizador en su búsqueda de los valores mínimos encontrados.



```{python}

# Implementación del método de descenso por gradiente
x0 = np.array([300*(2*np.random.random()-1),300*(2*np.random.random()-1)]) # generación de un punto aleatorio
xs = [x0]
for i in range(25):
    x_new = optimizador_mult_numdev(Schwefel, x0)
    xs.append(x_new)
    x0 = x_new

# Definición de la función graficadora
def graficar(frame):
    x, y = zip(*xs[:frame+1])
    line.set_data(x, y)
    return line,

# Animación final
ani = animation.FuncAnimation(fig, graficar, frames=len(xs), interval=300)
ani

```

### Optimización en 3D

```{python}
!pip install plotly
```

```{python}
np.random.seed(42)
x0 = np.array([300*(2*np.random.random()-1) for _ in range(3)])  # Punto inicial en 3D
print(f"Punto inicial: {x0}")

xs = [x0]  # Almacenará todos los puntos por los que pasa el optimizador
for i in range(25):
    x_new = optimizador_mult_numdev(Schwefel, x0, eta=0.0001, tol=1e-6, max_iter=10000)
    xs.append(x_new)
    x0 = x_new

xs = np.array(xs)  # Convertir a un array de Numpy para facilitar la manipulación

```

```{python}
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Trayectoria del optimizador
ax.plot(xs[:,0], xs[:,1], xs[:,2], marker='o', linestyle='-', color='r', label='Trayectoria del Optimizador')

ax.set_title('Trayectoria del Optimizador de Descenso por Gradiente en la Función de Schwefel')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
ax.legend()

plt.show()

```

```{python}
import plotly.graph_objs as go
import numpy as np

# Definir la función de Schwefel para tres dimensiones
def Schwefel(x):
    return 418.9829 * len(x) - sum([xi * np.sin(np.sqrt(abs(xi))) for xi in x])

# Generar datos para la superficie de la función de Schwefel
x = np.linspace(-500, 500, 100)
y = np.linspace(-500, 500, 100)
X, Y = np.meshgrid(x, y)
Z = np.array([Schwefel([X[i][j], Y[i][j], 0]) for i in range(X.shape[0]) for j in range(X.shape[1])])
Z = Z.reshape(X.shape)

# Generar el punto optimizado (aquí necesitas tus valores optimizados)
x_opt = [420.9687, 420.9687, 0]  # Reemplaza con el resultado de tu optimización
z_opt = Schwefel(x_opt)

# Crear la figura
fig = go.Figure(data=[
    go.Surface(z=Z, x=X, y=Y, colorscale='Viridis'),
    go.Scatter3d(x = xs[:,0], y = xs[:,1], z = xs[:,2], mode='markers', marker=dict(size=5, color='red'))
])

# Actualizar el layout de la figura
fig.update_layout(title='Función de Schwefel con Punto Optimizado en 3D', autosize=False,
                  width=700, height=700,
                  margin=dict(l=65, r=50, b=65, t=90))

# Mostrar la figura
fig.show()

```

```{python}
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def Schwefel(x):
    return 418.9829 * len(x) - sum(x[i] * np.sin(np.sqrt(abs(x[i]))) for i in range(len(x)))

def num_grad(x, fun, h=1e-4):
    return np.array([(fun(x + e * h) - fun(x - e * h)) / (2 * h) for e in np.eye(len(x))])

def optimizador_mult_numdev(f, x0, eta=0.0001, tol=1e-6, max_iter=1e4):
    x = np.array(x0)
    iter = 0
    while np.linalg.norm(num_grad(x, f)) > tol and iter < max_iter:
        x = x - eta * num_grad(x, f)
        iter += 1
    return x

np.random.seed(42)
# Punto inicial aleatorio en 3D
x0 = np.array([500*(2*np.random.random()-1) for _ in range(3)])
x_opt = optimizador_mult_numdev(Schwefel, x0, eta=0.0001, tol=1e-6, max_iter=10000)

# Generar una malla de puntos para graficar la función
x = np.linspace(-500, 500, 100)
y = np.linspace(-500, 500, 100)
X, Y = np.meshgrid(x, y)
Z = np.array([Schwefel([X[i][j], Y[i][j]]) for i in range(X.shape[0]) for j in range(X.shape[1])])
Z = Z.reshape(X.shape)

# Graficar la función en 3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', alpha=0.8)

# Graficar el punto optimizado
ax.scatter(x_opt[0], x_opt[1], Schwefel(x_opt), color='r', s=50, label='Punto optimizado')

ax.set_title('Función de Schwefel y Punto Optimizado')
ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('Valor de función')
ax.legend()

plt.show()

```


## 3. Optimización mediante algoritmos evolutivos

### **- Modificación de las funciones de prueba iniciales**

Aquí modificamos las definiciones de las funciones de prueba para que se ajusten a nuestro algoritmo evolutivo


```{python}
# Se implementa la función de Rastrigin vectorizada para arreglos numpy
def Rastrigin_ga(X,solutionidx, ga_instance):
  Y = 10 * len(X) + np.sum(X ** 2 - 10 * np.cos(2 * np.pi * X))
  fitness = -Y
  return (fitness)

# Se implementa la función de Schwefel vectorizada para arreglos numpy
def Schwefel_ga(X,solutionidx, ga_instance):
    Y = np.sum(X * np.sin(np.sqrt(np.abs(X))))
    fitness = -Y
    return(fitness)
```

### 3.1 Función de Rastrigin

#### - Optimización de la función de Rastrigin

Aquí se crea una instancia de nuestro algoritmo genético usando pygad para optimizar la función de Rastrigin.

Especificaciones de los parámetros:

* Se usa un número de generaciones de 60
* Se emparejan 2 individuos en cada generación
* La función de ajuste será nuestra función de prueba
* Se tomarán 9 individuos en cada generación
* El criterio de selección de reproducción : mejores individuos
* Tipo de mutación: Aleatoria


```{python}
# Corrección: Asegúrate de que la función acepte tres parámetros: la instancia de GA, una solución y su índice.
def Rastrigin_ga(ga_instance, solution, solution_idx):
    Y = 10 * len(solution) + np.sum(solution ** 2 - 10 * np.cos(2 * np.pi * solution))
    fitness = -Y  # El negativo se usa porque PyGAD busca maximizar la aptitud, y Rastrigin se minimiza.
    return fitness

  
# Configuración de la instancia GA ajustada a PyGAD 2.20.0
ga_instance_rastrigin = pygad.GA(num_generations=60,
                                 num_parents_mating=2,
                                 fitness_func=Rastrigin_ga,
                                 sol_per_pop=9,
                                 num_genes=2,
                                 init_range_low=-5,
                                 init_range_high=4,
                                 parent_selection_type="sss",
                                 keep_parents=1,
                                 crossover_type="single_point",
                                 mutation_type="random",
                                 mutation_percent_genes=10,
                                 save_solutions=True)
```

Una vez creada la instancia de nuestro algoritmo, ejecutamos la optimización mediante el algoritmo evolutivo y mostramos la evolución del ajuste generación tras generación



```{python}
ga_instance_rastrigin.run()
soluciones_rastrigin = ga_instance_rastrigin.solutions
ga_instance_rastrigin.plot_fitness()
plt.show()
```

#### - Muestra de gráficos

Generamos un gráfico nuevamente para nuestra función de prueba


```{python}
# Se implementa la función de Rastrigin vectorizada para arreglos numpy
def Rastrigin_ga(X,solutionidx):
  Y = 10 * len(X) + np.sum(X ** 2 - 10 * np.cos(2 * np.pi * X))
  fitness = -Y
  return (fitness)

  # Particionamiento del rango de cada variable
ncols = 100
nrows = 100
X = np.linspace(-9.12, 9.12, ncols)  #Posibles valores de X
Y = np.linspace(-9.12, 9.12, nrows)  #Posibles valores de Y
X, Y = np.meshgrid(X, Y)  #Definición del plano con posibles valores

# Evaluación de la función de Rastrigin
Z = [-Rastrigin_ga(np.array([X[i,j], Y[i,j]]),1) for i in range(nrows) for j in range(ncols)]
Z = np.array(Z).reshape([nrows,ncols])

fig, ax = plt.subplots()
fig.set_size_inches(10,7)
contorno = ax.contour(X,Y,Z)  #Curvas de nivel de la función de Rastrigin
contornof = ax.contourf(X,Y,Z) #Rellenando las curvas de nivel
line, = ax.plot([], [], 'xr--',mec='b') #Creando la línea que contendrá los puntos
              # de la animación de Optimización
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Función de Rastrigin')
plt.colorbar(contornof) # Se muestra la barra lateral con la escala de valores
          # ¡¡Éste método depende de la creación de un objeto "contourf" !!
plt.show()
```

Y animamos los resultados obtenidos después de aplicar el algoritmo evolutivo


```{python}
# Definición de la función graficadora
def graficar(frame):
    x, y = zip(*soluciones_rastrigin[:frame+1])
    line.set_data(x, y)
    return line,

# Animación final
ani = animation.FuncAnimation(fig, graficar, frames=len(xs), interval=300)
ani
```

### 3.2 Función de Schwefel


#### - Optimización de la función de Schwefel

Aquí se crea una instancia de nuestro algoritmo genético usando pygad para optimizar la función de Schwefel.

Especificaciones de los parámetros:

* Se usa un número de generaciones de 60
* Se emparejan 2 individuos en cada generación
* La función de ajuste será nuestra función de prueba
* Se tomarán 9 individuos en cada generación
* El criterio de selección de reproducción : mejores individuos
* Tipo de mutación: Aleatoria

```{python}
import numpy as np
import pygad
import matplotlib.pyplot as plt

# Ajustamos la función de Schwefel para que acepte los tres parámetros requeridos.
def Schwefel_ga(ga_instance, solution, solution_idx):
    Y = np.sum(solution * np.sin(np.sqrt(np.abs(solution))))
    fitness = -Y  # Negativo ya que buscamos minimizar la función de Schwefel
    return fitness

# Configuración de la instancia de GA con los ajustes necesarios
ga_instance_schwefel = pygad.GA(num_generations=80,
                                num_parents_mating=2,
                                fitness_func=Schwefel_ga,
                                sol_per_pop=9,
                                num_genes=30,
                                init_range_low=-500,
                                init_range_high=500,
                                parent_selection_type="sss",
                                keep_parents=1,
                                crossover_type="single_point",
                                mutation_type="random",
                                mutation_percent_genes=10,
                                save_solutions=True)



```

Una vez creada la instancia de nuestro algoritmo, en este caso para Schwefel, ejecutamos la optimización mediante el algoritmo evolutivo y mostramos la evolución del ajuste generación tras generación


```{python}
# Ejecutar la instancia de GA
ga_instance_schwefel.run()

# Obtener y mostrar la mejor solución
solution, solution_fitness, solution_idx = ga_instance_schwefel.best_solution()
print(f"Best Solution: {solution}")
print(f"Best Solution Fitness: {solution_fitness}")

# Opcional: Graficar la evolución de la fitness a lo largo de las generaciones
ga_instance_schwefel.plot_fitness()

# Mostrar el gráfico (en caso de estar usando Jupyter Notebook o un entorno que lo permita)
plt.show()
```

#### - Muestra de gráficos

Generamos un gráfico nuevamente para nuestra función de prueba


```{python}
soluciones_schwefel = ga_instance_schwefel.best_solution()
# Se implementa la función de Schwefel vectorizada para arreglos numpy
def Schwefel_ga(X,solutionidx):
    Y = np.sum(X * np.sin(np.sqrt(np.abs(X))))
    fitness = -Y
    return(fitness)
# Particionamiento del rango de cada variable
ncols = 100
nrows = 100
X = np.linspace(-512, 512, ncols)  #Posibles valores de X
Y = np.linspace(-512, 512, nrows)  #Posibles valores de Y
X, Y = np.meshgrid(X, Y)  #Definición del plano con posibles valores

# Evaluación de la función de Rastrigin
Z = [Schwefel_ga(np.array([X[i,j], Y[i,j]]),1) for i in range(nrows) for j in range(ncols)]
Z = np.array(Z).reshape([nrows,ncols])

fig, ax = plt.subplots()
fig.set_size_inches(10,7)
contorno = ax.contour(X,Y,Z)  #Curvas de nivel de la función de Rastrigin
contornof = ax.contourf(X,Y,Z) #Rellenando las curvas de nivel
line, = ax.plot([], [], 'xr--',mec='b') #Creando la línea que contendrá los puntos
              # de la animación de Optimización
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_title('Función de Schwefel')
plt.colorbar(contornof) # Se muestra la barra lateral con la escala de valores
          # ¡¡Éste método depende de la creación de un objeto "contourf" !!
plt.show()
```

Y animamos los resultados obtenidos después de aplicar el algoritmo evolutivo


```{python}

soluciones_schwefel = (soluciones_schwefel[0],)

# Ahora pasas soluciones_schwefel a tu función de animación
# Suponiendo que 'fig' y 'line' ya están definidos correctamente en tu código
ani = animation.FuncAnimation(fig, lambda frame: graficar(soluciones_schwefel), frames=23, interval=300)
ani
```

